Имеет смысл лишь на уровне процессора. Как процессор интепретирует несколько байт.
	В файле порядок байт не имеет начения.
	Имеет значение в контексте процессора, когда многобайтное число будет интерпретированно и будет выполняться операция.

	Big Endian (прямой порядок, от старшего к младшему) - хранит старший байт в младшем адресе, а младший байт в старшем(большем) адресе. Начинается старшим байтом и заканчивается младшим байтом.
		0х1234
		big-endian начинает с самой большой цифры

	Little Endian (обратный порядок) - хранит младший байт в малдшем адресе.
		0х3412
		little-endian – с самой маленькой. 


	Т.е. если читать из файла однобайтовые значения то, никаких проблем не возникает. Но если читать многобайтовые значения/числа (допустим int т.е. 4Б) то тут воникает важность в порядке.

	Когда данные пишутся из памяти в файл или читаются из файла в память (напрямую, без участия процессора т.е. без мат.операций или сравнений), байты передаются в порядке возрастания адресов.
	С другой стороны, операции сравнения и сортировки требуют работы с кодами как с числами. В общем – либо проблемы с порядком байтов в файлах, либо проблемы с алгоритмами обработки.


		Big-endian хранит старший байт числа по меньшему адресу памяти, что интуитивно понятно при чтении слева направо. Например, число 0x12345678 будет храниться как 12 34 56 78. (Порядок адресации слево направо).
		В то время как little-endian делает обратное, помещая младший байт по меньшему адресу, так что то же число будет храниться как 78 56 34 12


	В сетевых протоколах обычно используется big-endian порядок, так как он считается более "естественным" для чтения чисел слева направо. Однако, большинство современных ПК используют little-endian архитектуру, что создает необходимость в конвертации данных при отправке их через сеть. Это важно учитывать при разработке программного обеспечения, которое взаимодействует с сетью
		Big-endian is the dominant order in network protocols and is referred to as network order.
		Стандартом в сети является порядок от старшего к младшему. Т.е. старший байт идёт первым или в младшем адресе.

		Систему, совместимую с процессорами x86, называют little endian


	Как унать какой порядок в системе?
		Нужно сохранить многобайтовое число а потом прочитать его по одному байту.

	Для определения порядка в файле используют маркер последовательности:
		Byte Order Mark — BOM
			0xFEFF - если считанное значение соответствует 0xFEFF, то порядок соответствует, если не соответствует, то порядок отличается. (по-русски называется «неразрывный пробел нулевой ширины» FEFF в 16, он же 65279 в 10)
			0xFFFE - собствено для LittleEndian


	Имя байта            W         X          Y          Z
	Позиция (адрес)      0         1          2          3
	Значение (hex)      0x12      0x34       0x56       0x78

	Чтение по одному байту
		void *p = 0;   // p указатель на неизвестный тип данных
	    		       // p нулевой указатель - не разыменовывать
		char *c;       // c указатель на один байт


		c = (char *)p; 	// Этот оператор говорит компьютеру, что р указывает на то же место, и данные по этому адресу нужно интерпретировать как один символ (1 байт). 
						// В этом случае, с будет указывать на память по адресу 0, или на байт W.
						// Если мы выведем с, то получим значение, хранящееся в W, которое равно шестнадцатеричному 0x12 (помните, что W — это полный байт). Этот пример не зависит от типа компьютера — опять же, все компьютеры одинаково хорошо понимают, что же такое один байт


	Теперь давайте рассмотрим пример с многобайтными данными (наконец-то!). Короткая сводка: “short int” это 2-х байтовое число (16 бит), которое может иметь значение от 0 до 65535 (если оно беззнаковое). Давайте используем его в примере.

		short *s; 		// указатель на short int (2 байта)
		s = 0;    		// указатель на позицию 0; *s это значение
						// Итак, s это указатель на short int, и сейчас он указывает на позицию 0 (в которой хранится W).

			- Машина с прямым порядком хранения: short int состоит из двух байт. Позиция s это адрес 0 (W или 0х12), а позиция s + 1 это адрес 1 (X или 0х34). 
				Поскольку первый байт является старшим, то число должно быть следующим 256 * байт 0 + байт 1 или 256 * W + X, или же 0х1234.
			- Машина с обратным порядком хранения: short int состоит из 2 байт. Позиция s со значение 0х12 и позиция s + 1 со значением 0х34. 
				Но в моем мире первым является младший байт! И число должно быть байт 0 + 256 * байт 1 или 256 * X + W, или 0х3412.

		int *i; // указатель на int (4 байты 32-битовой машине)
		i = 0;  // указывает на позицию 0, а *i значение по этому адресу		
				
			- Машина с прямым порядком хранения: тип int состоит из 4 байт и первый байт является старшим. Считываю 4 байта (WXYZ) из которых старший W. Полученное число: 0х12345678.
			- Машина с обратным порядком хранения: несомненно, int состоит из 4 байт, но старшим является последний. Так же считываю 4 байта (WXYZ), но W будет расположен в конце — так как он является младшим. Полученное число: 0х78563412.

Кодировка:
	Для UTF-8 не различается порядок байт.
		Символы utf-8 получаются из Unicode следующим образом (из Википедии):
			0x00000000 — 0x0000007F: 0xxxxxxx (т.о. символы 0-127 не меняются)
			0x00000080 — 0x000007FF: 110xxxxx 10xxxxxx
			0x00000800 — 0x0000FFFF: 1110xxxx 10xxxxxx 10xxxxxx
			0x00010000 — 0x001FFFFF: 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx

			Т.е. если символ занимает двадцать бит - то будет заполнено 20ть позиций х

		Еще один плюс utf-8 — в компьютерах с любой архитектурой байты хранятся по возрастанию адресов и поэтому не нужны BE/LE (суррогатные пары, кстати, тоже не нужны). 
		Счастливым пользователям «чисто латинского» алфавита достаточно 1 байта.
		А для кириллицы всегда надо два байта (как в utf-16).


	Для UTF-16 есть два вида записил в BigEndian и в LittleEndian
		+ В UTF-16 символы в диапазоне U+0000-U+FFFF используются 16-битовое представление (основная многоязыковая плоскость)

			- Символы с номерами 0-55295(0x0000-0хD7FF) и 57344-65_535 (0xE000-0xFFFF) кодируются с помощью 16 бит без изменений (без использования префиксов)
					--- Исключенный отсюда диапазон (0xD800-0xDFFF) используется как раз для кодирования так называемых суррогатных пар — символов, которые кодируются двумя 16-битными словами.
				 - Рассмотрим пример кодирования буквы «П».
				 	- Юникод соответствует номер 1055 -- (0х041F) -- 0000 0100 0001 1111 -- (П)
				 	- Номера букв русского и английского алфавитов таблицы Юникод передаются без изменений при помощи 16 бит, старшие незначащие биты принимают нулевое значение.
			- А остальные символы, номера которых в двоичном представлении формируются количеством бит больше 16, кодируются 32 битами с использованием специального алгоритма(см. ниже).


		+ А для символов U+10000-U+10FFFF в используются «суррогатная пара» из кодов в области U+D800-U+DFFF
			???? Перед переводом числа U+10000 и выше его уменьшают на 0х10000 и полученное 20-битное число кодируют суррогатной парой.
			- Символы вне основной плоскости, как тетраграмматон, означающий центр (U+1D306), можно закодировать в UTF-16 только двумя 16-битными кодовыми единицами: 0xD834 0xDF06. Это называется суррогатной парой. 
			- Обратите внимание, что суррогатная пара представляет только один символ.
				- Первая кодовая единица суррогатной пары всегда находится в диапазоне от 0xD800 до 0xDBFF и называется верхней частью пары.
				- Вторая кодовая единица суррогатной пары всегда находится в диапазоне от 0xDC00 до 0xDFFF и называется нижней частью пары.

					Юникодовский символ «Куча дерьма» (U+1F4A9) в UTF-16 придётся кодировать как суррогатную пару, т. е. два суррогата. Чтобы преобразовать любую кодовую точку в суррогатную пару, используйте такой алгоритм (на JavaScript). Имейте в виду, что мы используем шестнадцатеричную нотацию.

						var High_Surrogate = function(Code_Point){ return Math.floor((Code_Point - 0x10000) / 0x400) + 0xD800 };
							// 0x10000 = 65_536 - это нижняя граница диапазона привышающего 16 бит.
							// 0х400 это 1024 (т.е. 2 в 10ой т.е. на десять бит вправо смещаем оставляем старшие 10 бит)
							
						var Low_Surrogate  = function(Code_Point){ return (Code_Point - 0x10000) % 0x400 + 0xDC00 }; 
							// Напротив, оставляет всё что в младших 10 битах.

						 // Reverses The Conversion
						 var Code_Point = function(High_Surrogate, Low_Surrogate){
							return (High_Surrogate - 0xD800) * 0x400 + Low_Surrogate - 0xDC00 + 0x10000;
						 };

						 // Вычетание 0x10000 позволяет привести к 20ти битам. Здесь не магия. Просто максимальное число/код имеет значение U+10FFFF и вычитая из него 0x10000 получаем число в  20бит
						 // Деление на 0х400 даёт выделение старших 10 бит
						 // Остаток от деления на 0х400 даёт выделение младших 10 бит


			Номер предложенного символа в таблице Юникод – 68620 (0х10COC).
			Алгоритм преобразования номера символа в код UTF-16 состоит из нескольких шагов:
				--- Из значения номера символа вычесть число 0х10000. Данная операция позволяет привести размерность бинарного представления номера символа к 20 битам. Для предложенного символа получим: 0х10COC – 0x10000 = 0xC0C.
					----- Из кода символа вычитается 0х10000. В результате получится значение от нуля до 0хFFFFF, которое помещается в разрядную сетку 20 бит.
				--- Для полученного значения выделить старшие 10 бит и младшие 10 бит. В примере число 0хС0С в бинарном виде представляется, как 0000000011_0000001100, где первая часть 10 старших бит, а вторая часть – 10 младших.
				- К шестнадцатеричному значению 0xD800 (11011000 00000000) прибавить значение 0х03 (00000000 00000011), сформированное 10 старшими битами, полученными на предыдущем шаге. 0xD800 + 0х03 =  0хD803 (11011000 00000011) – 16 старших бит кодового слова UTF-16.
				--- К шестнадцатеричному значению 0xDC00 (11011000 00000000) прибавить значение 0х0C (00000000 00001100), сформированное 10 младшими битами, полученными на шаге №2. 0xDС00 + 0х0С =  DС0С (11011100 00001100) – 16 младших бит кодового слова UTF-16.
				--- Кодовое слово UTF-16, соответствующее символу в примере, формируется из бит, полученных на шагах 3 и 4: 0хD803DC0C (11011000 00000011 11011100 00001100).
						Нужно обраттить внимание на первые части обоих 16ричных чисел - это "1101" видимо они и определяют что это суррогаты. См. выше где описан исключённый диапазон.